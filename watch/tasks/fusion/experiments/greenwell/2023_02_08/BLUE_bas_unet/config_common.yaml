data:
  time_steps: 5
  chip_dims: 128
  # channels: "(WV):blue|green|red"
  channels: "(L8,S2):blue|green|red"
  window_space_scale: 10GSD
  input_space_scale: 10GSD
  output_space_scale: 10GSD
  batch_size: 16
  chip_overlap: 0
  dist_weights: 0
  min_spacetime_weight: 0.5
  neg_to_pos_ratio: 0.25
  normalize_inputs: true
  normalize_perframe: false
  resample_invalid_frames: true
  temporal_dropout: 0.
  time_sampling: hardish3
  time_span: 6m
  upweight_centers: true
  use_centered_positives: false
  use_grid_positives: true
  verbose: 1
  max_epoch_length: 3200
  mask_low_quality: true
  mask_samecolor_method: null
model:
  class_path: watch.tasks.fusion.methods.UNetBaseline
  init_args:
    token_dim: 32
    global_change_weight: 0.0
    global_class_weight: 0.0
    global_saliency_weight: 1.0
    saliency_loss: focal
optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 1e-4
    weight_decay: 1e-4
    betas:
      - 0.9
      - 0.99
# lr_scheduler:
#   class_path: torch.optim.lr_scheduler.OneCycleLR
#   init_args:
#     max_lr: 0.001
#     total_steps: 10000
#     anneal_strategy: "linear"
#     pct_start: 0.5
profile: false
seed_everything: 1234
trainer:
  accumulate_grad_batches: 8
  callbacks:
    # - class_path: watch.utils.lightning_ext.callbacks.AutoResumer
    # - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    #   init_args:
    #     monitor: val_class_f1_macro
    #     mode: max
    #     save_top_k: 5
    #     auto_insert_metric_name: true
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        monitor: val_loss
        mode: min
        save_top_k: 5
        auto_insert_metric_name: true
  check_val_every_n_epoch: 1
  enable_checkpointing: true
  enable_model_summary: true
  enable_progress_bar: true
  # gradient_clip_algorithm: norm
  # gradient_clip_val: 0.5
  log_every_n_steps: 5
  logger: true
  max_steps: 50000
  num_sanity_val_steps: 2
  replace_sampler_ddp: true
  track_grad_norm: 2
